# üéôÔ∏è OpenAI Realtime API Implementation Guide - Part 2
## Tools, Functions, and Best Practices

---

## 7Ô∏è‚É£ **FUNCTION/TOOL SCHEMAS**

### **Complete Tools Array:**

```typescript
const TOOLS_SCHEMA = [
  {
    type: 'function',
    name: 'searchKnowledgeBase',
    description: 'Search knowledge base for DSCR loan information',
    parameters: {
      type: 'object',
      properties: {
        query: { type: 'string', description: 'Search query' },
        topK: { type: 'number', description: 'Results to return', default: 3 }
      },
      required: ['query']
    }
  },
  {
    type: 'function',
    name: 'analyzeDeal',
    description: 'Calculate DSCR, cash flow, and ROI for a property',
    parameters: {
      type: 'object',
      properties: {
        purchasePrice: { type: 'number' },
        downPaymentPercent: { type: 'number' },
        interestRate: { type: 'number' },
        loanTermYears: { type: 'number' },
        monthlyRent: { type: 'number' },
        propertyTaxRate: { type: 'number', default: 1.2 },
        insurance: { type: 'number', default: 150 },
        vacancyRate: { type: 'number', default: 5 }
      },
      required: ['purchasePrice', 'downPaymentPercent', 'interestRate', 'loanTermYears', 'monthlyRent']
    }
  },
  {
    type: 'function',
    name: 'saveLead',
    description: 'Save lead to database after getting consent',
    parameters: {
      type: 'object',
      properties: {
        leadDraft: {
          type: 'object',
          properties: {
            name: { type: 'string' },
            email: { type: 'string' },
            phone: { type: 'string' },
            productType: { 
              type: 'string',
              enum: ['dscr', 'hard_money', 'fix_flip', 'portfolio']
            },
            consentGiven: { type: 'boolean' }
          },
          required: ['name', 'email', 'phone', 'productType', 'consentGiven']
        }
      },
      required: ['leadDraft']
    }
  },
  {
    type: 'function',
    name: 'scoreLead',
    description: 'Score lead and generate offer (auto-called after saveLead)',
    parameters: {
      type: 'object',
      properties: {
        leadId: { type: 'string' }
      },
      required: ['leadId']
    }
  },
  {
    type: 'function',
    name: 'scheduleCall',
    description: 'Provide scheduling link',
    parameters: {
      type: 'object',
      properties: {
        leadId: { type: 'string' }
      },
      required: ['leadId']
    }
  },
  {
    type: 'function',
    name: 'perplexitySearch',
    description: 'Search real-time market data',
    parameters: {
      type: 'object',
      properties: {
        query: { type: 'string' }
      },
      required: ['query']
    }
  }
];
```

---

## 8Ô∏è‚É£ **TOOL EXECUTION**

```typescript
const handleFunctionCall = async (name: string, args: any, callId: string) => {
  console.log(`[Tool] ${name}`);
  
  let result: any;
  
  try {
    switch (name) {
      case 'searchKnowledgeBase':
        const kbResponse = await fetch('/api/knowledge-search', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(args)
        });
        result = await kbResponse.json();
        break;
      
      case 'perplexitySearch':
        const pxResponse = await fetch('/api/perplexity-search', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(args)
        });
        result = await pxResponse.json();
        break;
      
      case 'analyzeDeal':
        const dealResponse = await fetch('/api/analyze-deal', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(args)
        });
        result = await dealResponse.json();
        break;
      
      case 'saveLead':
      case 'scoreLead':
      case 'scheduleCall':
        // Dedicated voice tools endpoint
        const voiceResponse = await fetch('/api/voice/tools', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ tool: name, args })
        });
        const data = await voiceResponse.json();
        result = data.result;
        break;
      
      default:
        result = { error: `Unknown function: ${name}` };
    }
  } catch (error) {
    result = { error: error.message };
  }
  
  // Send result back to OpenAI
  if (wsRef.current?.readyState === WebSocket.OPEN) {
    wsRef.current.send(JSON.stringify({
      type: 'conversation.item.create',
      item: {
        type: 'function_call_output',
        call_id: callId,
        output: JSON.stringify(result)
      }
    }));
    
    // Request new response
    wsRef.current.send(JSON.stringify({ type: 'response.create' }));
  }
};
```

---

## 9Ô∏è‚É£ **EVENT MESSAGE HANDLING**

```typescript
const handleRealtimeMessage = async (message: any) => {
  switch (message.type) {
    // Session events
    case 'session.created':
      console.log('‚úÖ Session created');
      break;
    
    case 'session.updated':
      console.log('‚úÖ Session configured');
      break;
    
    // Input audio events
    case 'input_audio_buffer.speech_started':
      stopAllAudio();
      setIsSpeaking(false);
      if (activeResponseRef.current && wsRef.current) {
        wsRef.current.send(JSON.stringify({ type: 'response.cancel' }));
        activeResponseRef.current = false;
      }
      setIsRecording(true);
      break;
    
    case 'input_audio_buffer.speech_stopped':
      break;
    
    case 'input_audio_buffer.committed':
      if (wsRef.current?.readyState === WebSocket.OPEN) {
        wsRef.current.send(JSON.stringify({ type: 'response.create' }));
      }
      break;
    
    // Response events
    case 'response.created':
      activeResponseRef.current = true;
      stopAllAudio();
      setIsSpeaking(true);
      break;
    
    case 'response.done':
      activeResponseRef.current = false;
      if (message.response?.status === 'failed') {
        console.error('Response failed:', message.response.status_details);
        setError('Response generation failed');
      }
      setIsSpeaking(false);
      break;
    
    case 'response.cancelled':
      activeResponseRef.current = false;
      setIsSpeaking(false);
      break;
    
    // Audio output events
    case 'response.audio.delta':
    case 'response.audio_transcript.delta':
      if (message.delta || message.audio) {
        await playAudioChunk(message.delta || message.audio);
      }
      break;
    
    // Text output events
    case 'response.text.delta':
      if (message.delta) {
        setTranscript(prev => {
          const lastItem = prev[prev.length - 1];
          if (lastItem?.role === 'assistant') {
            return [...prev.slice(0, -1), { 
              ...lastItem, 
              text: lastItem.text + message.delta 
            }];
          }
          return [...prev, { 
            role: 'assistant', 
            text: message.delta, 
            timestamp: new Date() 
          }];
        });
      }
      break;
    
    // Function call events
    case 'response.function_call_arguments.done':
      await handleFunctionCall(
        message.name, 
        JSON.parse(message.arguments), 
        message.call_id
      );
      break;
    
    // Error events
    case 'error':
      const errorCode = message.error?.code;
      
      // Suppress benign errors
      if (errorCode === 'response_cancel_not_active' || 
          errorCode === 'conversation_already_has_active_response') {
        console.log('‚ö†Ô∏è Benign error (expected)');
        break;
      }
      
      // Real errors
      console.error('API error:', message.error);
      setError(message.error?.message || 'Connection error');
      break;
    
    case 'rate_limits.updated':
      // Ignore
      break;
    
    default:
      console.log('Unhandled:', message.type);
  }
};
```

---

## üîü **ERROR HANDLING**

### **Categories:**

**Benign Errors (Suppress):**
```typescript
- 'response_cancel_not_active'
- 'conversation_already_has_active_response'

// These are race conditions during interruptions - expected behavior
```

**Real Errors (Show User):**
```typescript
- WebSocket connection failed
- Authentication failed
- Microphone access denied
- Rate limit exceeded
- Function execution failed
```

### **Error Recovery:**

```typescript
let reconnectAttempts = 0;
const MAX_RECONNECT = 3;

ws.onclose = () => {
  if (reconnectAttempts < MAX_RECONNECT) {
    reconnectAttempts++;
    setTimeout(() => {
      console.log(`Reconnecting (${reconnectAttempts})...`);
      connectToRealtimeAPI();
    }, 2000 * reconnectAttempts);
  } else {
    setError('Connection lost. Please refresh.');
  }
};
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ **BEST PRACTICES**

### **‚úÖ DO:**

1. **Use Server-side VAD**
```typescript
turn_detection: { type: 'server_vad' }
```
More reliable than client-side detection.

2. **Track Audio Sources**
```typescript
const audioSourcesRef = useRef<AudioBufferSourceNode[]>([]);
```
Essential for clean interruptions.

3. **Track Response State**
```typescript
const activeResponseRef = useRef<boolean>(false);
```
Prevents unnecessary cancellations.

4. **Schedule Audio**
```typescript
source.start(nextPlayTimeRef.current);
nextPlayTimeRef.current += audioBuffer.duration;
```
Ensures seamless playback without gaps.

5. **Separate Audio Contexts**
```typescript
const audioContextRef = useRef<AudioContext | null>(null);     // Input
const playbackContextRef = useRef<AudioContext | null>(null);  // Output
```
Prevents feedback loops.

6. **Keep Prompts Concise**
```typescript
// Under 500 words for system prompt
// Under 3 sentences for responses
```

7. **Enable Tool Chaining**
```typescript
// AI automatically calls scoreLead after saveLead
```

### **‚ùå DON'T:**

1. **Don't Use Client-side VAD**
- Browser compatibility issues
- Unreliable detection

2. **Don't Share AudioContext**
- Causes feedback
- Audio quality issues

3. **Don't Cancel Without Checking State**
- Causes API errors
- Clutters console

4. **Don't Play Audio Immediately**
- Causes gaps
- Unprofessional quality

5. **Don't Write Long Prompts**
- AI gets confused
- Slower responses

---

## 1Ô∏è‚É£2Ô∏è‚É£ **REPLICATION CHECKLIST**

### **Phase 1: Setup (1-2 days)**
- [ ] OpenAI API key with Realtime access
- [ ] Next.js or React project
- [ ] Environment variables configured
- [ ] Test WebSocket connection

### **Phase 2: Basic Voice (2-3 days)**
- [ ] WebSocket to Realtime API
- [ ] Microphone capture (PCM16, 24kHz)
- [ ] Audio playback
- [ ] Basic conversation working
- [ ] UI controls (connect/disconnect)

### **Phase 3: Custom Tools (3-5 days)**
- [ ] Business-specific system prompt
- [ ] Function/tool schemas defined
- [ ] Tool execution endpoints
- [ ] Test tool calling
- [ ] Transcript display

### **Phase 4: Production (5-7 days)**
- [ ] Interruption handling
- [ ] Error recovery
- [ ] Database integration
- [ ] Email notifications
- [ ] Audio optimization
- [ ] Analytics tracking

### **Phase 5: Polish (3-5 days)**
- [ ] Mobile responsive
- [ ] Browser compatibility
- [ ] Performance optimization
- [ ] Error messages
- [ ] User testing

**Total: 2-3 weeks for production-ready**

---

## 1Ô∏è‚É£3Ô∏è‚É£ **TECHNICAL REQUIREMENTS**

### **Required:**
```
‚úì OpenAI API Key (Realtime API access)
‚úì WebSocket support
‚úì Web Audio API support
‚úì Microphone permission
‚úì Modern browser (Chrome/Edge)
```

### **Audio Specs:**
```
Format: PCM16
Sample Rate: 24kHz
Channels: Mono (1)
Bit Depth: 16-bit
Encoding: Base64
```

### **Browser Support:**
```
‚úÖ Chrome 80+
‚úÖ Edge 80+
‚úÖ Safari 14+
‚ö†Ô∏è Firefox (limited Web Audio support)
‚ùå IE (not supported)
```

---

## 1Ô∏è‚É£4Ô∏è‚É£ **PERFORMANCE BENCHMARKS**

### **Expected Metrics:**
```
Voice Latency: <500ms
Audio Quality: Professional
Interruption Response: Instant
Function Execution: 1-3s
Memory Usage: <50MB
CPU Usage: <20%
```

### **Optimization Tips:**
```typescript
// 1. Reuse contexts
if (!playbackContextRef.current) {
  playbackContextRef.current = new AudioContext({ sampleRate: 24000 });
}

// 2. Batch processing
const processor = audioContext.createScriptProcessor(4096, 1, 1);

// 3. Clean up resources
source.onended = () => {
  audioSourcesRef.current.splice(index, 1);
};
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ **BACKEND ENDPOINT (Voice Tools)**

```typescript
// /api/voice/tools/route.ts
import { NextRequest, NextResponse } from 'next/server';
import { prisma } from '@/lib/db';
import { scoreLead } from '@/lib/scoring';
import { sendEmail } from '@/lib/email';

export async function POST(req: NextRequest) {
  const { tool, args } = await req.json();
  
  let result: any;
  
  switch (tool) {
    case 'saveLead':
      const lead = await prisma.lead.create({
        data: {
          ...args.leadDraft,
          channel: 'voice_chat'
        }
      });
      result = { leadId: lead.id, status: 'created' };
      break;
    
    case 'scoreLead':
      const lead = await prisma.lead.findUnique({ 
        where: { id: args.leadId } 
      });
      const score = await scoreLead(lead);
      
      await prisma.lead.update({
        where: { id: args.leadId },
        data: { score: score.score, offer: score.offer }
      });
      
      if (score.disposition === 'qualified') {
        await sendEmail(lead, score);
      }
      
      result = score;
      break;
    
    case 'scheduleCall':
      result = {
        meetingUrl: 'https://calendly.com/your-link',
        message: 'Scheduling link ready'
      };
      break;
  }
  
  return NextResponse.json({ success: true, result });
}
```

---

## 1Ô∏è‚É£6Ô∏è‚É£ **KEY TAKEAWAYS**

### **What Makes It Work:**
1. ‚úÖ Server-side VAD
2. ‚úÖ Audio source tracking
3. ‚úÖ Response state management
4. ‚úÖ Scheduled audio playback
5. ‚úÖ Proper error handling
6. ‚úÖ Tool chaining

### **Common Pitfalls:**
1. ‚ùå Client-side VAD
2. ‚ùå Shared audio contexts
3. ‚ùå Unchecked cancellations
4. ‚ùå Immediate audio playback
5. ‚ùå Long system prompts

### **Production Checklist:**
- ‚úÖ Clean interruptions
- ‚úÖ No console errors
- ‚úÖ Database integration
- ‚úÖ Email notifications
- ‚úÖ Mobile responsive
- ‚úÖ Error recovery

---

## üìû **SUPPORT RESOURCES**

- OpenAI Realtime API Docs: https://platform.openai.com/docs/guides/realtime
- Web Audio API: https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API
- WebSocket API: https://developer.mozilla.org/en-US/docs/Web/API/WebSocket

---

**This implementation is production-tested and powers Cap's voice-to-voice conversations with 100+ real users.** üöÄ
